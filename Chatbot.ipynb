{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sparadrap1101/Blockchain_Chatbot/blob/main/Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Blockchain Chatbot\n",
        "\n",
        "*Machine Learning Project - Alexis Cerio*\n",
        "\n",
        "#### Welcome to my Deep Learning Chatbot project!\n",
        "This chatbot aims to answer basic questions about Blockchain using Deep Learning.\n",
        "\n",
        "A quick set up is require to use this chatbot in Colab, follow the instructions:\n",
        "\n",
        "- First run the import code section below."
      ],
      "metadata": {
        "id": "P95aGpSMFTcV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NLGxDYOJro9",
        "outputId": "c0af6335-7c55-4bb2-b860-cef05cd2d81c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tflearn in /usr/local/lib/python3.8/dist-packages (0.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from tflearn) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from tflearn) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tflearn) (1.21.6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# We import and install all required libraries here.\n",
        "import numpy\n",
        "!pip install tflearn\n",
        "import tflearn\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "import tensorflow\n",
        "import random\n",
        "import json\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Then go to [the initial repo](https://github.com/Sparadrap1101/Blockchain_Chatbot) and download `files.zip`.\n",
        "\n",
        "- After unzipping `files.zip` on your computer, add all 6 files in the file section on the left on Colab in order to use my trained model.\n",
        "\n",
        "- Then, run the next section:"
      ],
      "metadata": {
        "id": "dAdEJeTZP1Ck"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PW11X3UpHTzA",
        "outputId": "7612fca8-a4fc-4222-de20-9df590e288d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oui\n",
            "Oui\n"
          ]
        }
      ],
      "source": [
        "# We start by opening our 'dataset.json' file which contains the chatbot dataset.\n",
        "with open(\"dataset.json\") as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Then we verify if we already have trained our model and we load the arrays if it's the case.\n",
        "try:\n",
        "    print(\"Oui\")\n",
        "    with open(\"data.pickle\", \"rb\") as f:\n",
        "        words, labels, training, output = pickle.load(f)\n",
        "\n",
        "# If we haven't trained our model yet, we create new arrays and start to fill them with the dataset.\n",
        "except:\n",
        "    print(\"Non\")\n",
        "    words = []\n",
        "    labels = []\n",
        "    docs_x = []\n",
        "    docs_y = []\n",
        "\n",
        "    # We look on our dataset, get the patterns (i.e. questions), and tokenize them in different words. We then store them and their tags.\n",
        "    for intent in data[\"intents\"]:\n",
        "        for pattern in intent[\"patterns\"]:\n",
        "            tokenizedWords = nltk.word_tokenize(pattern)\n",
        "            words.extend(tokenizedWords)\n",
        "            docs_x.append(tokenizedWords)\n",
        "            docs_y.append(intent[\"tag\"])\n",
        "\n",
        "        # Then we store our tags in the labels array.\n",
        "        if intent[\"tag\"] not in labels:\n",
        "            labels.append(intent[\"tag\"])\n",
        "\n",
        "    # We stem and sort these different words to simplify utilisation. We also sort our labels.\n",
        "    words = [stemmer.stem(word.lower()) for word in words if word != \"?\"]\n",
        "    words = sorted(list(set(words)))\n",
        "    labels = sorted(labels)\n",
        "\n",
        "\n",
        "    # In this section we will check which words of a pattern are in our global words list, translate that in numbers and add those in arrays.\n",
        "    training = []\n",
        "    output = []\n",
        "\n",
        "    # We create an empty array with the good lenght.\n",
        "    emptyOutput = [0 for _ in range(len(labels))]\n",
        "\n",
        "    for index, doc in enumerate(docs_x):\n",
        "        bag = []\n",
        "\n",
        "        stemmedWords = [stemmer.stem(word.lower()) for word in doc]\n",
        "\n",
        "        # If the words in the global list is in the pattern, we append 1 in the bag[] array. Else we append 0.\n",
        "        for word in words:\n",
        "            if word in stemmedWords:\n",
        "                bag.append(1)\n",
        "            else:\n",
        "                bag.append(0)\n",
        "\n",
        "        outputRow = emptyOutput[:]\n",
        "        outputRow[labels.index(docs_y[index])] = 1\n",
        "\n",
        "        # We then append the bag in the training array and the outputRow in the output array.\n",
        "        training.append(bag)\n",
        "        output.append(outputRow)\n",
        "\n",
        "    # We transform these arrays in numpy array to use it later.\n",
        "    training = numpy.array(training)\n",
        "    output = numpy.array(output)\n",
        "\n",
        "    # When finished, we store our arrays in a pickle file in order reuse them another time without doing all this again.\n",
        "    with open(\"data.pickle\", \"wb\") as f:\n",
        "        pickle.dump((words, labels, training, output), f)\n",
        "\n",
        "# Finally, we set up our neural network here. We have chosen 3 layers of 8 neurons fully connected for this model.\n",
        "tensorflow.compat.v1.reset_default_graph()\n",
        "net = tflearn.input_data(shape=[None, len(training[0])])\n",
        "net = tflearn.fully_connected(net, 8)\n",
        "net = tflearn.fully_connected(net, 8)\n",
        "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\")\n",
        "net = tflearn.regression(net)\n",
        "\n",
        "model = tflearn.DNN(net)\n",
        "\n",
        "# If our model has already been trained, we load it.\n",
        "try:\n",
        "  print(\"Oui\")\n",
        "  model.load(\"model.tflearn\")\n",
        "\n",
        "# If it hasn't, we start training it (can be long). \n",
        "# Then we save the model to be able to use it another time without having to train it again.\n",
        "except:\n",
        "  print(\"Non\")\n",
        "  model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)\n",
        "  model.save(\"model.tflearn\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Finally run the last section and enjoy chatting with the bot about Blockchain stuff!"
      ],
      "metadata": {
        "id": "KmNsG8pzStk6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0iQT5D7jH4tu",
        "outputId": "ae007f2b-f42f-438a-e89c-75b487cb3791"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start talking with the bot (type exit to stop)!\n",
            "\n",
            "You: Hello\n",
            "Bot: Bonjour, comment allez-vous ? \n",
            "\n",
            "You: what's your name ?\n",
            "Bot: I'm Bob, a language model for Blockchain questions. \n",
            "\n",
            "You: what are you doing ?\n",
            "Bot: I.m Bob, your bot assistant \n",
            "\n",
            "You: explain me bitcoin\n",
            "Bot: Bitcoin is a cryptocurrency that operates on a decentralized network, allowing for secure and transparent transfer of funds without the need for intermediaries. It is the first and largest cryptocurrency by market capitalization. \n",
            "\n",
            "You: explain me ethereum\n",
            "Bot: Ethereum is a blockchain platform that allows developers to build decentralized applications and smart contracts. It has its own cryptocurrency called Ether (ETH) and operates on a decentralized network. \n",
            "\n",
            "You: I don't understand impermanent loss\n",
            "Bot: AMMs provide a more streamlined and user-friendly experience but may have lower liquidity compared to order book DEXs. Order book DEXs have higher liquidity and better price discovery, but may have longer wait times for trades to fill and a steeper learning curve for new users. \n",
            "\n",
            "You: What's impermanent loss ?\n",
            "Bot: Ripple is a blockchain-based digital payment protocol that enables fast and secure cross-border transactions. Its cryptocurrency, XRP, is used to facilitate cross-currency transactions and to provide liquidity to financial institutions. \n",
            "\n",
            "You: impermanent loss\n",
            "Bot: Ethereum is a decentralized blockchain platform that provides the infrastructure for creating and running smart contracts and decentralized applications. Its cryptocurrency, Ether (ETH), is used to pay for transactions and computational services on the network. \n",
            "\n",
            "You: impermanent loss\n",
            "Bot: Ethereum is a blockchain platform that allows developers to build decentralized applications and smart contracts. It has its own cryptocurrency called Ether (ETH) and operates on a decentralized network. \n",
            "\n",
            "You: impermanent Loss in Defi ?\n",
            "Bot: Therefore, it is important to consider the potential for impermanent loss when trading liquidity in an AMM platform, and to weigh the potential rewards against the potential risks. \n",
            "\n",
            "You: oracle\n",
            "Bot: Bitcoin is a digital currency that uses cryptography for security and operates on a decentralized network, offering a secure and transparent alternative to traditional currencies and financial systems. \n",
            "\n",
            "You: DeFi\n",
            "Bot: Gas Limit is a variable in the Ethereum network that sets a maximum limit on the amount of computational effort that can be expended in executing a single transaction or smart contract. The purpose of the gas limit is to prevent the network from being congested by excessive computation, which could cause it to become slow or even grind to a halt. The gas limit is adjusted dynamically based on network conditions, with the goal of balancing the need for processing power with the need for network efficiency. \n",
            "\n",
            "You: vampire attack\n",
            "Bot: A Vampire Attack is a type of attack that occurs on DeFi platforms where a malicious actor drains liquidity from a pool by repeatedly withdrawing and depositing assets. The impact of a Vampire Attack is that it can lead to reduced liquidity, higher slippage, and increased volatility for the affected pool, as well as potential losses for liquidity providers. A Vampire Attack can occur due to a vulnerability in the smart contract code that allows an attacker to manipulate the calculations of the pool's balance and rewards, or due to a lack of rate limiting or anti-spam measures in the contract. \n",
            "\n",
            "You: attack 51% ?\n",
            "Bot: A 51% attack is a type of attack that occurs when a single entity or a group of entities control more than 51% of the computing power on a blockchain network. This allows the attackers to manipulate the network by controlling the majority of the network's hash rate, enabling them to reverse transactions, double-spend coins, and prevent new transactions from being confirmed. The impact of a 51% attack can be severe, as it undermines the integrity of the blockchain and the confidence of its users. A 51% attack can occur due to centralization of the network's hash rate, where a single entity controls a large portion of the network's mining power, or due to a combination of factors such as compromised nodes or rented hash power. \n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-52d0f9066bd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Bot:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-52d0f9066bd9>\u001b[0m in \u001b[0;36mchat\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# We make a loop for a discussion continue until the user write 'exit'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             )\n\u001b[0;32m--> 860\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "# Here is our wordsBag() function, it helps us werify which words of the user question correspond to the words in our array of words.\n",
        "def wordsBag(userSentence, words):\n",
        "    bag = [0 for _ in range(len(words))]\n",
        "\n",
        "    # We tokenize and stem them.\n",
        "    userWords = nltk.word_tokenize(userSentence)\n",
        "    userWords = [stemmer.stem(word.lower()) for word in userWords]\n",
        "\n",
        "    # If a word in the sentence correspond to a word of our dataset, we put 1 on bag[] array to the correct index.\n",
        "    for userWord in userWords:\n",
        "        for index, word in enumerate(words):\n",
        "            if word == userWord:\n",
        "                bag[index] = 1\n",
        "    \n",
        "    # We then return this array to help our model making his prediction.\n",
        "    return numpy.array(bag)\n",
        "\n",
        "# Finally, the chat function from where the user will interact with the bot.\n",
        "def chat():\n",
        "    print(\"Start talking with the bot (type exit to stop)!\\n\")\n",
        "\n",
        "    # We make a loop for a discussion continue until the user write 'exit'.\n",
        "    while True:\n",
        "        inp = input(\"You: \")\n",
        "        if inp.lower() == \"exit\":\n",
        "            break\n",
        "\n",
        "        # From our wordsBag() function, our list of words and the input of the user, we make a prediction \n",
        "        # with our model in order to find the best answer.\n",
        "        results = model.predict([wordsBag(inp, words)])[0]\n",
        "\n",
        "        # We keep only the most probable answer and we get his index to get the tag of the answer we want.\n",
        "        indexResults = numpy.argmax(results)\n",
        "        tag = labels[indexResults]\n",
        "\n",
        "        for tags in data[\"intents\"]:\n",
        "            if tags['tag'] == tag:\n",
        "                responses = tags['responses']\n",
        "\n",
        "        # We print a random in the 2/3 possible responses to have some variance for the user and not always the same response for the same question.\n",
        "        print(\"Bot:\", random.choice(responses), \"\\n\")\n",
        "\n",
        "chat()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOq0IvAFkoNj2/puzULn9Nd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}